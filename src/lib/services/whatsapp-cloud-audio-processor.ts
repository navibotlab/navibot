import { PrismaClient, Prisma } from '@prisma/client';
import OpenAI from 'openai';
import fetch from 'node-fetch';
import { structuredAudioLog } from './whatsapp-cloud-logger';
import { formatAudioTranscription } from './whatsapp-cloud-formatter';
import { audioCache } from './whatsapp-cloud-cache';
import { getSupabaseClient } from '../supabase';
import { compressAudio, isAudioMimeTypeSupported } from './whatsapp-cloud-audio-compressor';
import { LogContext } from './whatsapp-cloud-logger';

/**
 * Ajusta o fuso hor√°rio para compensar a convers√£o autom√°tica para UTC
 * Subtrai 3 horas para o fuso hor√°rio Brasil (UTC-3)
 */
function adjustTimezoneBR(): Date {
  const date = new Date();
  date.setHours(date.getHours() - 3);
  return date;
}

// Tipos MIME de √°udio permitidos
const ALLOWED_AUDIO_MIME_TYPES = [
  'audio/ogg',
  'audio/mpeg',
  'audio/mp4',
  'audio/webm',
  'audio/wav',
  'audio/m4a',
  'audio/aac',
  'audio/flac',
  'audio/mp3',
  'audio/mp4',
  'audio/oga'
  
];

// Define a interface para par√¢metros de processamento de √°udio
interface ProcessAudioParams {
  message: {
    type: string;
    audio?: {
      id: string;
      mime_type?: string;
      voice?: boolean;
      duration?: number;
    };
  };
  threadId: string | null;
  mediaUrl: string;
  correlationId: string;
}

// Interface para o contexto de execu√ß√£o
interface WhatsAppCloudProcessorContext {
  prisma: PrismaClient;
  openai: OpenAI;
  accessToken: string;
  workspaceId: string;
  assistant_id?: string;
}

/**
 * Classe respons√°vel pelo processamento de √°udios do WhatsApp Cloud API
 */
export class WhatsAppCloudAudioProcessor {
  private prisma: PrismaClient;
  private openai: OpenAI;
  private accessToken: string;
  private workspaceId: string;
  private assistant_id?: string;

  constructor(context: WhatsAppCloudProcessorContext) {
    this.prisma = context.prisma;
    this.openai = context.openai;
    this.accessToken = context.accessToken;
    this.workspaceId = context.workspaceId;
    this.assistant_id = context.assistant_id;
  }

  /**
   * Valida e processa um √°udio, armazenando-o no Supabase
   * Retorna a URL p√∫blica do √°udio armazenado
   */
  async validateAndProcessAudio(mediaUrl: string, mediaId: string, mimeType?: string, duration?: number): Promise<string> {
    const logContext: LogContext = {
      correlationId: mediaId,
      messageId: mediaId,
      phone: '',
      type: 'audio'
    };

    try {
      // Verificar cache primeiro
      const cachedAudio = audioCache.get<string>(mediaId);
      if (cachedAudio) {
        structuredAudioLog(logContext, 'info', 'üéØ √Åudio encontrado no cache');
        return cachedAudio;
      }

      // Gerar nome de arquivo e caminho
      const contentType = mimeType || 'audio/ogg';
      const extension = contentType.split('/')[1]?.split(';')[0] || 'ogg';
      const fileName = `whatsapp_${mediaId}.${extension}`;
      const filePath = `${this.workspaceId}/${fileName}`;
      
      // Verificar se o arquivo j√° existe no Supabase
      try {
        const supabase = getSupabaseClient();
        const { data: existingFile } = await supabase.storage
          .from('audios')
          .list(this.workspaceId, { 
            search: fileName,
            limit: 1
          });
          
        if (existingFile && existingFile.length > 0) {
          structuredAudioLog(logContext, 'info', 'üéØ Arquivo encontrado no Supabase, gerando URL');
          const { data: { publicUrl } } = supabase.storage
            .from('audios')
            .getPublicUrl(filePath);
            
          // Armazenar no cache
          audioCache.set(mediaId, publicUrl);
          return publicUrl;
        }
      } catch (existingCheckError) {
        structuredAudioLog(logContext, 'warn', '‚ö†Ô∏è Erro ao verificar exist√™ncia do arquivo no Supabase, continuando');
      }

      // Baixar √°udio do WhatsApp
      structuredAudioLog(logContext, 'info', 'üì• Baixando √°udio do WhatsApp...');
      let audioBuffer: ArrayBuffer;
      let finalContentType = contentType;
      
      try {
        const response = await fetch(mediaUrl, {
          headers: { Authorization: `Bearer ${this.accessToken}` }
        });

        if (!response.ok) {
          throw new Error(`Erro ao baixar √°udio: ${response.statusText}`);
        }

        // Obter tipo MIME e validar
        finalContentType = response.headers.get('content-type') || contentType;
        structuredAudioLog(logContext, 'info', 'üì¶ Tipo MIME detectado:', finalContentType);
        
        if (!isAudioMimeTypeSupported(finalContentType, ALLOWED_AUDIO_MIME_TYPES)) {
          throw new Error(`Tipo de √°udio n√£o suportado: ${finalContentType}`);
        }

        // Obter o buffer do √°udio
        audioBuffer = await response.arrayBuffer();
        structuredAudioLog(logContext, 'info', '‚úÖ √Åudio baixado com sucesso:', { 
          size: audioBuffer.byteLength, 
          type: finalContentType 
        });
      } catch (downloadError) {
        structuredAudioLog(logContext, 'error', '‚ùå Erro fatal ao baixar √°udio:', downloadError);
        throw downloadError; // N√£o pode continuar sem o buffer
      }
      
      // "Compress√£o" do √°udio (pode ser apenas passagem direta)
      let processedBuffer: Buffer;
      try {
        structuredAudioLog(logContext, 'info', 'üîÑ Processando √°udio...');
        processedBuffer = await compressAudio(audioBuffer, finalContentType, {
          bitrate: '64k',
          channels: 1,
          sampleRate: 22050
        }, logContext);
        
        if (processedBuffer.length < audioBuffer.byteLength) {
          structuredAudioLog(logContext, 'info', '‚úÖ √Åudio comprimido com sucesso', {
            originalSize: audioBuffer.byteLength,
            compressedSize: processedBuffer.length,
            reduction: `${Math.round((1 - processedBuffer.length / audioBuffer.byteLength) * 100)}%`
          });
        } else {
          structuredAudioLog(logContext, 'info', '‚ÑπÔ∏è Usando √°udio original sem compress√£o');
        }
      } catch (compressionError) {
        // Se a compress√£o falhar, usamos o buffer original
        structuredAudioLog(logContext, 'warn', '‚ö†Ô∏è Erro ao processar √°udio, usando buffer original:', compressionError);
        processedBuffer = Buffer.from(audioBuffer);
      }

      // Upload para o Supabase
      let publicUrl = '';
      try {
        structuredAudioLog(logContext, 'info', 'üì§ Fazendo upload para o Supabase...');
        const supabase = getSupabaseClient();
        
        const { data, error } = await supabase.storage
          .from('audios')
          .upload(filePath, processedBuffer, {
            contentType: finalContentType,
            upsert: true
          });

        if (error) {
          throw new Error(`Erro ao fazer upload para o Supabase: ${error.message}`);
        }

        const { data: { publicUrl: url } } = supabase.storage
          .from('audios')
          .getPublicUrl(filePath);

        publicUrl = url;
        structuredAudioLog(logContext, 'info', '‚úÖ Upload conclu√≠do com sucesso:', { url: publicUrl });
      } catch (uploadError) {
        structuredAudioLog(logContext, 'error', '‚ùå Erro ao fazer upload para o Supabase:', uploadError);
        throw uploadError;
      }

      // Armazenar no cache
      audioCache.set(mediaId, publicUrl);
      structuredAudioLog(logContext, 'info', '‚úÖ Processamento de √°udio conclu√≠do');
      
      return publicUrl;
    } catch (error) {
      structuredAudioLog(logContext, 'error', '‚ùå Erro ao processar √°udio:', error);
      throw error;
    }
  }

  /**
   * Processa uma mensagem de √°udio do WhatsApp
   */
  async processAudioMessage(
    message: ProcessAudioParams['message'],
    threadId: string | null,
    mediaUrl: string,
    correlationId: string
  ): Promise<string> {
    try {
      structuredAudioLog({ correlationId, messageId: message.audio?.id }, 'info', '\n=== IN√çCIO DO PROCESSAMENTO DE √ÅUDIO ===');
      structuredAudioLog({ correlationId, messageId: message.audio?.id }, 'info', 'Detalhes do √°udio:', {
        mediaUrl,
        mimeType: message.audio?.mime_type,
        voice: message.audio?.voice,
        duration: message.audio?.duration
      });

      // Processar o √°udio para o Supabase e obter a URL p√∫blica
      let publicAudioUrl = mediaUrl; // Inicializar com a URL original
      
      try {
        if (message.audio?.id) {
          structuredAudioLog({ correlationId, messageId: message.audio?.id }, 'info', 'üîÑ Processando √°udio para armazenamento permanente...');
          publicAudioUrl = await this.validateAndProcessAudio(
            mediaUrl, 
            message.audio.id, 
            message.audio.mime_type, 
            message.audio.duration
          );
          structuredAudioLog({ correlationId, messageId: message.audio?.id }, 'info', '‚úÖ URL p√∫blica do √°udio gerada:', publicAudioUrl);
        }
      } catch (storageError) {
        structuredAudioLog({ correlationId, messageId: message.audio?.id }, 'warn', '‚ö†Ô∏è Erro ao processar √°udio para armazenamento. Continuando com URL original:', storageError);
        // Continuar com a URL original em caso de erro
      }

      // 1. Download do √°udio
      structuredAudioLog({ correlationId, messageId: message.audio?.id }, 'info', 'üì• Baixando √°udio...');
      const audioResponse = await fetch(mediaUrl, {
        headers: { Authorization: `Bearer ${this.accessToken}` }
      });

      if (!audioResponse.ok) {
        structuredAudioLog({ correlationId, messageId: message.audio?.id }, 'error', '‚ùå Erro ao baixar √°udio:', {
          status: audioResponse.status,
          statusText: audioResponse.statusText
        });
        throw new Error(`Falha ao baixar o √°udio: ${audioResponse.statusText}`);
      }

      const audioBuffer = await audioResponse.arrayBuffer();
      structuredAudioLog({ correlationId, messageId: message.audio?.id }, 'info', '‚úÖ √Åudio baixado com sucesso:', {
        size: audioBuffer.byteLength,
        type: message.audio?.mime_type
      });

      // 2. Transcri√ß√£o com Whisper
      structuredAudioLog({ correlationId, messageId: message.audio?.id }, 'info', 'üîÑ Iniciando transcri√ß√£o com Whisper...');
      try {
        // Determinar o tipo MIME adequado
        const contentType = message.audio?.mime_type || audioResponse.headers.get('content-type') || 'audio/ogg';
        structuredAudioLog({ correlationId, messageId: message.audio?.id }, 'info', 'üì¶ Tipo MIME detectado:', contentType);
        
        // Ajustar para formato compat√≠vel com Whisper
        let whisperMimeType = 'audio/ogg'; // Padr√£o
        
        if (contentType.includes('audio/ogg')) {
          whisperMimeType = 'audio/ogg';
        } else if (contentType.includes('audio/mpeg') || contentType.includes('audio/mp3')) {
          whisperMimeType = 'audio/mpeg';
        } else if (contentType.includes('audio/mp4')) {
          whisperMimeType = 'audio/mp4';
        } else if (contentType.includes('audio/wav')) {
          whisperMimeType = 'audio/wav';
        } else if (contentType.includes('webm')) {
          whisperMimeType = 'audio/webm';
        }
        
        structuredAudioLog({ correlationId, messageId: message.audio?.id }, 'info', 'üì¶ Tipo MIME para Whisper:', whisperMimeType);

        const audioBlob = new Blob([audioBuffer], { type: whisperMimeType });
        const audioFile = new File([audioBlob], 'audio.ogg', { type: whisperMimeType });
        
        structuredAudioLog({ correlationId, messageId: message.audio?.id }, 'info', 'üì¶ Arquivo de √°udio preparado:', {
          size: audioFile.size,
          type: audioFile.type,
          name: audioFile.name
        });

        const transcription = await this.openai.audio.transcriptions.create({
          file: audioFile,
          model: 'whisper-1',
          language: 'pt'
        });

        structuredAudioLog({ correlationId, messageId: message.audio?.id }, 'info', '\n=== TRANSCRI√á√ÉO DO √ÅUDIO ===');
        structuredAudioLog({ correlationId, messageId: message.audio?.id }, 'info', transcription.text);
        structuredAudioLog({ correlationId, messageId: message.audio?.id }, 'info', '===========================\n');

        // 3. Enviar a transcri√ß√£o para o agente vinculado
        if (!threadId) {
          throw new Error('Thread ID n√£o encontrado');
        }

        // Obter a conversa atual
        const conversation = await this.prisma.conversations.findFirst({
          where: { thread_id: threadId }
        });

        if (!conversation) {
          throw new Error('Conversa n√£o encontrada');
        }

        // Formatar a mensagem com √≠cone de play e transcri√ß√£o
        const formattedMessage = formatAudioTranscription(transcription.text);

        // Salvar a mensagem no banco de dados
        await this.prisma.messages.create({
          data: {
            conversation_id: conversation.id,
            content: transcription.text,
            sender: 'user',
            read: true,
            type: 'audio',
            media_url: publicAudioUrl,
            external_id: message.audio?.id,
            media_duration: message.audio?.duration,
            media_type: message.audio?.mime_type,
            created_at: adjustTimezoneBR(),
            updated_at: adjustTimezoneBR()
          }
        });

        // 4. Adicionar a mensagem √† thread do OpenAI
        structuredAudioLog({ correlationId, messageId: message.audio?.id }, 'info', 'ü§ñ Adicionando mensagem √† thread do OpenAI...');
        
        // Verificar se h√° alguma execu√ß√£o ativa na thread
        structuredAudioLog({ correlationId, messageId: message.audio?.id }, 'info', 'üîç Verificando execu√ß√µes ativas na thread antes de processar √°udio...');
        const activeRuns = await this.openai.beta.threads.runs.list(threadId, {
          limit: 1,
          order: 'desc'
        });
        
        // Se houver execu√ß√µes ativas, aguardar a conclus√£o
        if (activeRuns.data.length > 0) {
          const latestRun = activeRuns.data[0];
          
          // Verificar se a execu√ß√£o mais recente ainda est√° ativa
          if (['queued', 'in_progress', 'requires_action'].includes(latestRun.status)) {
            structuredAudioLog({ correlationId, messageId: message.audio?.id }, 'info', `‚è≥ Aguardando conclus√£o da execu√ß√£o ${latestRun.id} (status: ${latestRun.status})...`);
            
            // Esperar a conclus√£o da execu√ß√£o atual
            let runStatus = await this.openai.beta.threads.runs.retrieve(threadId, latestRun.id);
            while (['queued', 'in_progress', 'requires_action'].includes(runStatus.status)) {
              structuredAudioLog({ correlationId, messageId: message.audio?.id }, 'info', `üîÑ Execu√ß√£o ${latestRun.id} ainda ativa com status: ${runStatus.status}. Aguardando...`);
              await new Promise(resolve => setTimeout(resolve, 2000)); // Aguardar 2 segundos
              runStatus = await this.openai.beta.threads.runs.retrieve(threadId, latestRun.id);
            }
            
            structuredAudioLog({ correlationId, messageId: message.audio?.id }, 'info', `‚úÖ Execu√ß√£o anterior conclu√≠da com status: ${runStatus.status}`);
          }
        }
        
        await this.openai.beta.threads.messages.create(threadId, {
          role: 'user',
          content: transcription.text
        });

        // 5. Executar o assistant
        structuredAudioLog({ correlationId, messageId: message.audio?.id }, 'info', 'üîÑ Executando assistente com ID:', this.assistant_id);
        if (!this.assistant_id) {
          throw new Error('ID do assistente n√£o encontrado');
        }

        const run = await this.openai.beta.threads.runs.create(threadId, {
          assistant_id: this.assistant_id
        });

        // 6. Aguardar a conclus√£o do processamento
        structuredAudioLog({ correlationId, messageId: message.audio?.id }, 'info', '‚è≥ Aguardando resposta do assistente...');
        let runStatus = await this.openai.beta.threads.runs.retrieve(threadId, run.id);
        while (runStatus.status === 'queued' || runStatus.status === 'in_progress') {
          await new Promise(resolve => setTimeout(resolve, 1000));
          runStatus = await this.openai.beta.threads.runs.retrieve(threadId, run.id);
          structuredAudioLog({ correlationId, messageId: message.audio?.id }, 'info', 'üîÑ Status do processamento:', runStatus.status);
        }

        if (runStatus.status !== 'completed') {
          structuredAudioLog({ correlationId, messageId: message.audio?.id }, 'error', '‚ùå Falha na gera√ß√£o da resposta:', runStatus.status);
          throw new Error(`Execu√ß√£o falhou com status: ${runStatus.status}`);
        }

        // 7. Obter a resposta
        structuredAudioLog({ correlationId, messageId: message.audio?.id }, 'info', 'üì• Obtendo resposta gerada pelo assistente...');
        const messages = await this.openai.beta.threads.messages.list(threadId);
        const lastMessage = messages.data[0];
        
        if (!lastMessage || lastMessage.role !== 'assistant') {
          throw new Error('N√£o foi poss√≠vel obter a resposta do assistente');
        }

        const responseContent = lastMessage.content[0];
        if (responseContent.type !== 'text') {
          throw new Error('Resposta inesperada do assistente');
        }

        structuredAudioLog({ correlationId, messageId: message.audio?.id }, 'info', '\n=== RESPOSTA GERADA PELO AGENTE ===');
        structuredAudioLog({ correlationId, messageId: message.audio?.id }, 'info', responseContent.text.value);
        structuredAudioLog({ correlationId, messageId: message.audio?.id }, 'info', '=====================================\n');

        return responseContent.text.value;
      } catch (error: any) {
        structuredAudioLog({ correlationId, messageId: message.audio?.id }, 'error', '‚ùå Erro ao processar √°udio:', error);
        throw error; // Propagar o erro para ser tratado no n√≠vel superior
      }
    } catch (error: any) {
      structuredAudioLog({ correlationId, messageId: message.audio?.id }, 'error', '‚ùå Erro ao processar √°udio:', error);
      throw error; // Propagar o erro para ser tratado no n√≠vel superior
    }
  }
} 